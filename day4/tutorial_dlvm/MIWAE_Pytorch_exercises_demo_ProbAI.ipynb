{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MIWAE_Pytorch_exercises_demo_ProbAI.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjdYRutQxukL",
        "colab_type": "text"
      },
      "source": [
        "# Deep learning meets missing data: Doing it MIWAE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMq1ifa_uUna",
        "colab_type": "text"
      },
      "source": [
        "In this notebook, we'll show how to learn a deep generative model on a small and **incomplete** continuous data set. We will also show how to **impute** the missing values of this data set. \n",
        "\n",
        "This is based on the following paper, available [on arXiv](https://arxiv.org/abs/1812.02633):\n",
        "\n",
        "P.-A. Mattei & J. Frellsen, **MIWAE: Deep Generative Modelling and Imputation of Incomplete Data Sets**, *Proceedings of the 36th International Conference on Machine Learning*, PMLR 97:4413-4423, 2019.\n",
        "\n",
        "It is possible to run this notebook in Google Colab, which allows to benefit from free GPU computing.\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "    <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/pamattei/MIWAE_Pytorch_exercises_demo_ProbAI.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dawQVMWrvxYu",
        "colab_type": "text"
      },
      "source": [
        "# Installing and loading useful stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74ZkYf2YTn0s",
        "colab_type": "code",
        "outputId": "32af9c46-79a2-4e33-e4fa-75977d062dc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!pip3 install --user --upgrade scikit-learn # We need to update it to run missForest\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import scipy.stats\n",
        "import scipy.io\n",
        "import scipy.sparse\n",
        "from scipy.io import loadmat\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.distributions as td\n",
        "\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.impute import SimpleImputer"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.21.2)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.13.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.16.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SLCK6HRT7C2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mse(xhat,xtrue,mask): # MSE function for imputations\n",
        "    xhat = np.array(xhat)\n",
        "    xtrue = np.array(xtrue)\n",
        "    return np.mean(np.power(xhat-xtrue,2)[~mask])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0HjKh4WyB-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RpNVgHjuPzC",
        "colab_type": "text"
      },
      "source": [
        "# Loading the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeBa5C1WXoGN",
        "colab_type": "text"
      },
      "source": [
        "We'll use the Iris data set from scikit-learn:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIQ9W-Uv_ur0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "data = load_iris(True)[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTtS1FJQjw3P",
        "colab_type": "text"
      },
      "source": [
        "It is also possible to use the breast cancer or the Boston data sets by uncommenting one of the following cells:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RtDGEQjWNW1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from sklearn.datasets import load_breast_cancer\n",
        "#data = load_breast_cancer(True)[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qZQA7peYNI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from sklearn.datasets import load_boston\n",
        "#data = load_boston(True)[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxWks15H_GOB",
        "colab_type": "text"
      },
      "source": [
        "It is also possible to use the \"white wine\" or \"red wine\" UCI data sets by uncommenting one of the following cells:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ImVc9R52qdg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
        "#data = np.array(pd.read_csv(url, low_memory=False, sep=';'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akd4tc0m-HMz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\"\n",
        "#data = np.array(pd.read_csv(url, low_memory=False, sep=';'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiX9pkXkhG9Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt\"\n",
        "#data = np.array(pd.read_csv(url, low_memory=False, sep=','))[:,0:4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rp_Xbte_2zVK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xfull = (data - np.mean(data,0))/np.std(data,0)\n",
        "n = xfull.shape[0] # number of observations\n",
        "p = xfull.shape[1] # number of features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ct5EDgdXxJD",
        "colab_type": "text"
      },
      "source": [
        "We will remove uniformy at random 50% of the data. This corresponds to a *missing completely at random (MCAR)* scenario."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8A94e8tX8ta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(1234)\n",
        "\n",
        "perc_miss = 0.5 # 50% of missing data\n",
        "xmiss = np.copy(xfull)\n",
        "xmiss_flat = xmiss.flatten()\n",
        "miss_pattern = np.random.choice(n*p, np.floor(n*p*perc_miss).astype(np.int), replace=False)\n",
        "xmiss_flat[miss_pattern] = np.nan \n",
        "xmiss = xmiss_flat.reshape([n,p]) # in xmiss, the missing values are represented by nans\n",
        "mask = np.isfinite(xmiss) # binary mask that indicates which values are missing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uj0YaRppuvw4",
        "colab_type": "text"
      },
      "source": [
        "A simple way of imputing the incomplete data is to replace the missing values by zeros. This x_hat0 is what will be fed to our encoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PorBq9ncYZ8t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xhat_0 = np.copy(xmiss)\n",
        "xhat_0[np.isnan(xmiss)] = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "im4zpQifyI9S",
        "colab_type": "text"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqK000IKVW4L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "h = 128 # number of hidden units in (same for all MLPs)\n",
        "d = 1 # dimension of the latent space\n",
        "K = 20 # number of IS during training\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwcaqZ5qyNid",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srUk6d53ZCJT",
        "colab_type": "text"
      },
      "source": [
        "# Model building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zepRWE3kDI_-",
        "colab_type": "text"
      },
      "source": [
        "We will use a **deep latent variable model with a Gaussian prior and a Student's t observation model**. This can be written:\n",
        "\n",
        "$$p(\\mathbf{x}_1,...,\\mathbf{x}_n) = \\prod_{i=1}^n p(\\mathbf{x}_i|\\mathbf{z}_i)p(\\mathbf{z}_i),$$\n",
        "$$p(\\mathbf{z}_i) = \\mathcal{N}(\\mathbf{z}_i|\\mathbf{0}_d,\\mathbf{I}_d), $$\n",
        "$$p(\\mathbf{x}_i|\\mathbf{z}_i) = \\text{St} (\\mathbf{x}_i|\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}}(\\mathbf{z}_i),\\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}}(\\mathbf{z}_i),\\boldsymbol{\\nu}_{\\boldsymbol{\\theta}}(\\mathbf{z}_i)),$$\n",
        "\n",
        "where $\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}}: \\mathbb{R}^d \\rightarrow \\mathbb{R}^p$, $\\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}}: \\mathbb{R}^d \\rightarrow \\mathcal{S}_p^{++}$, and $\\boldsymbol{\\nu}_{\\boldsymbol{\\theta}}: \\mathbb{R}^d \\rightarrow \\mathbb{R}_+^p$ are functions parametrised by deep neural nets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQGrEj5flTeK",
        "colab_type": "text"
      },
      "source": [
        "The weights of these nets are stored in a parameter $\\boldsymbol{\\theta}$. We choose to use the following simple architecture, where the 3 neural nets share the first layers:\n",
        "$$f_{\\boldsymbol{\\theta}} (\\mathbf{z})=\\sigma(\\mathbf{W}_1\\sigma(\\mathbf{W}_0\\mathbf{z}+\\mathbf{b}_0)+\\mathbf{b}_1) $$\n",
        "\n",
        "$$\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}}(\\mathbf{z}) = \\mathbf{W}_\\boldsymbol{\\mu}f_{\\boldsymbol{\\theta}} (\\mathbf{z})+\\mathbf{b}_\\boldsymbol{\\mu}, $$\n",
        "\n",
        "$$\\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}}(\\mathbf{z}) = \\text{Diag}\\left(\\text{Softplus}(\\mathbf{W}_\\boldsymbol{\\sigma}f_{\\boldsymbol{\\theta}} (\\mathbf{z})+\\mathbf{b}_\\boldsymbol{\\sigma}) + 10^{-3}\\right), $$\n",
        "\n",
        "$$\\boldsymbol{\\nu}_{\\boldsymbol{\\theta}}(\\mathbf{z}) = \\text{Softplus}(\\mathbf{W}_\\boldsymbol{\\nu}f_{\\boldsymbol{\\theta}} (\\mathbf{z})+\\mathbf{b}_\\boldsymbol{\\nu}) + 3. $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8Tw89qNmLte",
        "colab_type": "text"
      },
      "source": [
        "A few **non-essential remarks** about this architecture:\n",
        "\n",
        "* This parametrisation is quite close to the one we use in the MIWAE paper. The main difference is that we use $\\sigma = \\text{ReLU}$ (which leads to faster training) while we used $\\sigma = \\text{tanh}$ in the paper.\n",
        "*   We use a [location-scale parametrisation](https://en.wikipedia.org/wiki/Location%E2%80%93scale_family) of the t distribution, following [the parametrisation available in TensorFlow](https://www.tensorflow.org/api_docs/python/tf/distributions/StudentT). Note in particular that $\\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}}(\\mathbf{z})$ is not the covariance matrix of $\\mathbf{x} | \\mathbf{z}$. When it exitsts, the actual covariance matrix is diagonal with diagonal  $$ \\frac{\\text{diag}(\\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}} (\\mathbf{z}))^2 \\boldsymbol{\\nu}_{\\boldsymbol{\\theta}}(\\mathbf{z})}{\\boldsymbol{\\nu}_{\\boldsymbol{\\theta}}(\\mathbf{z})-2}$$ (where all operations are made entrywise).\n",
        "*   The fact that the covariance matrix is diagonal means that we assume that **the features are independent conditionnally on the latent variable** (which is customary for DLVMs).\n",
        "* We add $3$ to the neural net that outputs the degrees of freedom. This is to guarantee that the tails of $p_{\\boldsymbol{\\theta}}(\\mathbf{x} | \\mathbf{z})$ are not too heavy. Indeed, having too heavy tails might imply that the mean of $p_{\\boldsymbol{\\theta}}(\\mathbf{x} | \\mathbf{z})$ does not exist! Adding 3 implies that the degrees of freedom is always larger than 3, implying in turn that **at least the first 3 moments of $p_{\\boldsymbol{\\theta}}(\\mathbf{x} | \\mathbf{z})$ are well-defined.**\n",
        "* We add $10^{-3}$ to the diagonal entries of $\\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}}$ to prevent singularities, [as advocated in our NeurIPS 2018 paper](https://papers.nips.cc/paper/7642-leveraging-the-exact-likelihood-of-deep-latent-variable-models). Why $10^{-3}$ specifically? Because, since the data have unit variance, this will imply that the latent variable explains at most $99.9999\\%$ of the variance of the data, which does not seem too restrictive. **This choice might be poor if the data are not standardised.**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yh_slL43JnLR",
        "colab_type": "text"
      },
      "source": [
        "We begin with the prior:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTzTVsO7XZTB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p_z = td.Independent(td.Normal(loc=torch.zeros(d).cuda(),scale=torch.ones(d).cuda()),1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KBnFeeMJ5S6",
        "colab_type": "text"
      },
      "source": [
        " Now, we define the **decoder**, which will be the backbone of the three functions $\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}}: \\mathbb{R}^d \\rightarrow \\mathbb{R}^p$, $\\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}}: \\mathbb{R}^d \\rightarrow \\mathcal{S}_p^{++}$, and $\\boldsymbol{\\nu}_{\\boldsymbol{\\theta}}: \\mathbb{R}^d \\rightarrow \\mathbb{R}_+^p$. Here, the output space of this decoder is $\\mathbb{R}^{3p}$. Some additional operations are needed for $\\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}}: \\mathbb{R}^d \\rightarrow \\mathcal{S}_p^{++}$, and $\\boldsymbol{\\nu}_{\\boldsymbol{\\theta}}: \\mathbb{R}^d \\rightarrow \\mathbb{R}_+^p$, but it'll be more convenient to implement them later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiCPYSprUtyA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder = nn.Sequential(\n",
        "    torch.nn.Linear(d, h),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(h, h),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(h, 3*p),  # the decoder will output both the mean, the scale, and the number of degrees of freedoms (hence the 3*p)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MgZJM8Yy8qG",
        "colab_type": "text"
      },
      "source": [
        "# Posterior approximation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHrpYUZGzCf2",
        "colab_type": "text"
      },
      "source": [
        "We will build a Gaussian posterior approximation $q(\\mathbf{z}|\\mathbf{x}) = \\mathcal{N} (\\mathbf{z}|\\mathbf{m}_\\gamma (\\mathbf{x}),\\mathbf{S}_\\gamma (\\mathbf{x}))$ by using an **encoder** that mimicks the architecture of the decoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kKPm4eiV3dl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = nn.Sequential(\n",
        "    torch.nn.Linear(p, h),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(h, h),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(h, 2*d),  # the encoder will output both the mean and the diagonal covariance\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxZQogI52Koa",
        "colab_type": "code",
        "outputId": "7accaff7-c109-47d7-ac59-fcc46f110afc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "encoder.cuda() # we'll use the GPU\n",
        "decoder.cuda()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=1, out_features=128, bias=True)\n",
              "  (1): ReLU()\n",
              "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (3): ReLU()\n",
              "  (4): Linear(in_features=128, out_features=12, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYzqx4syz2eT",
        "colab_type": "text"
      },
      "source": [
        "# Building the MIWAE loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TWrlHSU82Zj",
        "colab_type": "text"
      },
      "source": [
        "We will define a function that, given the imputation $\\iota(\\mathbf{x}^\\text{o})$ and the mask, computes the MIWAE bound."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnkox7-Q8hkA",
        "colab_type": "text"
      },
      "source": [
        "$$\n",
        "\\mathcal{L}_K (\\boldsymbol{\\theta,\\gamma}) = \\sum_{i=1}^n \\mathbb{E}_{\\mathbf{z}_{i1},\\ldots,\\mathbf{z}_{iK} \\sim q_{\\boldsymbol{\\gamma}}(\\mathbf{z}|\\mathbf{x}^\\text{o}_i)} \\left[ \\log\\frac{1}{K} \\sum_{k=1}^K \\frac{p_{\\boldsymbol{\\theta}}(\\mathbf{x}_i^\\text{o}|\\mathbf{z}_{ik})p(\\mathbf{z}_{ik})}{q_{\\boldsymbol{\\gamma}}(\\mathbf{z}_{ik}|\\mathbf{x}^\\text{o}_i)} \\right].\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P13f2MiGUXi3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def miwae_loss(iota_x,mask):\n",
        "  batch_size = iota_x.shape[0]\n",
        "  out_encoder = encoder(iota_x)\n",
        "  q_zgivenxobs = td.Independent(td.Normal(loc=out_encoder[..., :d],scale=torch.nn.Softplus()(out_encoder[..., d:(2*d)])),1)\n",
        "  \n",
        "  zgivenx = q_zgivenxobs.rsample([K])\n",
        "  zgivenx_flat = zgivenx.reshape([K*batch_size,d])\n",
        "  \n",
        "  out_decoder = decoder(zgivenx_flat)\n",
        "  all_means_obs_model = out_decoder[..., :p]\n",
        "  all_scales_obs_model = torch.nn.Softplus()(out_decoder[..., p:(2*p)]) + 0.001\n",
        "  all_degfreedom_obs_model = torch.nn.Softplus()(out_decoder[..., (2*p):(3*p)]) + 3\n",
        "  \n",
        "  data_flat = torch.Tensor.repeat(iota_x,[K,1]).reshape([-1,1])\n",
        "  tiledmask = torch.Tensor.repeat(mask,[K,1])\n",
        "  \n",
        "  all_log_pxgivenz_flat = torch.distributions.StudentT(loc=all_means_obs_model.reshape([-1,1]),scale=all_scales_obs_model.reshape([-1,1]),df=all_degfreedom_obs_model.reshape([-1,1])).log_prob(data_flat)\n",
        "  all_log_pxgivenz = all_log_pxgivenz_flat.reshape([K*batch_size,p])\n",
        "  \n",
        "  logpxobsgivenz = torch.sum(all_log_pxgivenz*tiledmask,1).reshape([K,batch_size])\n",
        "  logpz = p_z.log_prob(zgivenx)\n",
        "  logq = q_zgivenxobs.log_prob(zgivenx)\n",
        "  \n",
        "  neg_bound = -torch.mean(torch.logsumexp(logpxobsgivenz + logpz - logq,0))\n",
        "  \n",
        "  return neg_bound"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zl7ue5rDo1J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()),lr=1e-3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUWloElrESwP",
        "colab_type": "text"
      },
      "source": [
        "# Single imputation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-QpI8yN_aQl",
        "colab_type": "text"
      },
      "source": [
        "We can do single imputation using self normalised IS:\n",
        "\\begin{equation*}\n",
        "\\mathbb E [\\mathbf{x}^{\\textrm{m}} | \\mathbf{x}^{\\textrm{o}}] \\approx \\sum_{l=1}^L w_l \\, \\mathbf{x}^{\\textrm{m}}_{(l)},\n",
        "\\end{equation*}\n",
        "where $(\\mathbf{x}^{\\textrm{m}}_{(1)},\\mathbf{z}_{(1)}),\\ldots,(\\mathbf{x}^{\\textrm{m}}_{(L)},\\mathbf{z}_{(L)})$ are i.i.d.~samples from $p_{\\boldsymbol{\\theta}}(\\mathbf{x}^{\\textrm{m}}|\\mathbf{x}^{\\textrm{o}},\\mathbf{z})q_{\\boldsymbol{\\gamma}}(\\mathbf{z}|\\mathbf{x}^{\\textrm{o}})$ and \n",
        "\\begin{equation*}\n",
        "w_l=\\frac{r_l}{r_1+\\ldots+r_L}, \\; \\textrm{with} \\; r_l = \\frac{p_{\\boldsymbol{\\theta}}(\\mathbf{x}^{\\textrm{o}}|\\mathbf{z}_{(l)})p(\\mathbf{z}_{(l)})}{q_{\\boldsymbol{\\gamma}}(\\mathbf{z}_{(l)}|\\mathbf{x}^{\\textrm{o}})}.\n",
        "\\end{equation*}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ihzUc3iF731",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def miwae_impute(iota_x,mask,L):\n",
        "  batch_size = iota_x.shape[0]\n",
        "  out_encoder = encoder(iota_x)\n",
        "  q_zgivenxobs = td.Independent(td.Normal(loc=out_encoder[..., :d],scale=torch.nn.Softplus()(out_encoder[..., d:(2*d)])),1)\n",
        "  \n",
        "  zgivenx = q_zgivenxobs.rsample([L])\n",
        "  zgivenx_flat = zgivenx.reshape([L*batch_size,d])\n",
        "  \n",
        "  out_decoder = decoder(zgivenx_flat)\n",
        "  all_means_obs_model = out_decoder[..., :p]\n",
        "  all_scales_obs_model = torch.nn.Softplus()(out_decoder[..., p:(2*p)]) + 0.001\n",
        "  all_degfreedom_obs_model = torch.nn.Softplus()(out_decoder[..., (2*p):(3*p)]) + 3\n",
        "  \n",
        "  data_flat = torch.Tensor.repeat(iota_x,[L,1]).reshape([-1,1]).cuda()\n",
        "  tiledmask = torch.Tensor.repeat(mask,[L,1]).cuda()\n",
        "  \n",
        "  all_log_pxgivenz_flat = torch.distributions.StudentT(loc=all_means_obs_model.reshape([-1,1]),scale=all_scales_obs_model.reshape([-1,1]),df=all_degfreedom_obs_model.reshape([-1,1])).log_prob(data_flat)\n",
        "  all_log_pxgivenz = all_log_pxgivenz_flat.reshape([L*batch_size,p])\n",
        "  \n",
        "  logpxobsgivenz = torch.sum(all_log_pxgivenz*tiledmask,1).reshape([L,batch_size])\n",
        "  logpz = p_z.log_prob(zgivenx)\n",
        "  logq = q_zgivenxobs.log_prob(zgivenx)\n",
        "  \n",
        "  xgivenz = td.Independent(td.StudentT(loc=all_means_obs_model, scale=all_scales_obs_model, df=all_degfreedom_obs_model),1)\n",
        "\n",
        "  imp_weights = torch.nn.functional.softmax(logpxobsgivenz + logpz - logq,0) # these are w_1,....,w_L for all observations in the batch\n",
        "  xms = xgivenz.sample().reshape([L,batch_size,p])\n",
        "  xm=torch.einsum('ki,kij->ij', imp_weights, xms) \n",
        "  \n",
        "\n",
        "  \n",
        "  return xm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dODsWjNkBW_5",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROKruvy7FVP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def weights_init(layer):\n",
        "  if type(layer) == nn.Linear: torch.nn.init.orthogonal_(layer.weight)\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBn0XinLC4Uy",
        "colab_type": "code",
        "outputId": "9a8b6bf7-84c4-48c5-ba2c-91d278f2b807",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1445
        }
      },
      "source": [
        "miwae_loss_train=np.array([])\n",
        "mse_train=np.array([])\n",
        "mse_train2=np.array([])\n",
        "bs = 64 # batch size\n",
        "n_epochs = 2002\n",
        "xhat = np.copy(xhat_0) # This will be out imputed data matrix\n",
        "\n",
        "encoder.apply(weights_init)\n",
        "decoder.apply(weights_init)\n",
        "\n",
        "for ep in range(1,n_epochs):\n",
        "  perm = np.random.permutation(n) # We use the \"random reshuffling\" version of SGD\n",
        "  batches_data = np.array_split(xhat_0[perm,], n/bs)\n",
        "  batches_mask = np.array_split(mask[perm,], n/bs)\n",
        "  for it in range(len(batches_data)):\n",
        "    optimizer.zero_grad()\n",
        "    encoder.zero_grad()\n",
        "    decoder.zero_grad()\n",
        "    b_data = torch.from_numpy(batches_data[it]).float().cuda()\n",
        "    b_mask = torch.from_numpy(batches_mask[it]).float().cuda()\n",
        "    loss = miwae_loss(iota_x = b_data,mask = b_mask)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  if ep % 100 == 1:\n",
        "    print('Epoch %g' %ep)\n",
        "    print('MIWAE likelihood bound  %g' %(-np.log(K)-miwae_loss(iota_x = torch.from_numpy(xhat_0).float().cuda(),mask = torch.from_numpy(mask).float().cuda()).cpu().data.numpy())) # Gradient step      \n",
        "    \n",
        "    ### Now we do the imputation\n",
        "    \n",
        "    xhat[~mask] = miwae_impute(iota_x = torch.from_numpy(xhat_0).float().cuda(),mask = torch.from_numpy(mask).float().cuda(),L=10).cpu().data.numpy()[~mask]\n",
        "    err = np.array([mse(xhat,xfull,mask)])\n",
        "    mse_train = np.append(mse_train,err,axis=0)\n",
        "    print('Imputation MSE  %g' %err)\n",
        "    print('-----')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "MIWAE likelihood bound  -3.19261\n",
            "Imputation MSE  1.19521\n",
            "-----\n",
            "Epoch 101\n",
            "MIWAE likelihood bound  -1.80769\n",
            "Imputation MSE  0.476078\n",
            "-----\n",
            "Epoch 201\n",
            "MIWAE likelihood bound  -1.67583\n",
            "Imputation MSE  0.479187\n",
            "-----\n",
            "Epoch 301\n",
            "MIWAE likelihood bound  -1.54004\n",
            "Imputation MSE  0.444001\n",
            "-----\n",
            "Epoch 401\n",
            "MIWAE likelihood bound  -1.6617\n",
            "Imputation MSE  0.516101\n",
            "-----\n",
            "Epoch 501\n",
            "MIWAE likelihood bound  -1.51865\n",
            "Imputation MSE  0.512431\n",
            "-----\n",
            "Epoch 601\n",
            "MIWAE likelihood bound  -1.58895\n",
            "Imputation MSE  0.496381\n",
            "-----\n",
            "Epoch 701\n",
            "MIWAE likelihood bound  -1.63319\n",
            "Imputation MSE  0.4554\n",
            "-----\n",
            "Epoch 801\n",
            "MIWAE likelihood bound  -1.63603\n",
            "Imputation MSE  0.446688\n",
            "-----\n",
            "Epoch 901\n",
            "MIWAE likelihood bound  -1.4017\n",
            "Imputation MSE  0.493753\n",
            "-----\n",
            "Epoch 1001\n",
            "MIWAE likelihood bound  -1.45007\n",
            "Imputation MSE  0.51599\n",
            "-----\n",
            "Epoch 1101\n",
            "MIWAE likelihood bound  -1.42624\n",
            "Imputation MSE  0.478521\n",
            "-----\n",
            "Epoch 1201\n",
            "MIWAE likelihood bound  -1.40289\n",
            "Imputation MSE  0.467798\n",
            "-----\n",
            "Epoch 1301\n",
            "MIWAE likelihood bound  -1.51026\n",
            "Imputation MSE  0.523239\n",
            "-----\n",
            "Epoch 1401\n",
            "MIWAE likelihood bound  -1.51097\n",
            "Imputation MSE  0.432738\n",
            "-----\n",
            "Epoch 1501\n",
            "MIWAE likelihood bound  -1.43528\n",
            "Imputation MSE  0.425782\n",
            "-----\n",
            "Epoch 1601\n",
            "MIWAE likelihood bound  -1.51109\n",
            "Imputation MSE  0.402807\n",
            "-----\n",
            "Epoch 1701\n",
            "MIWAE likelihood bound  -1.43081\n",
            "Imputation MSE  0.431271\n",
            "-----\n",
            "Epoch 1801\n",
            "MIWAE likelihood bound  -1.43072\n",
            "Imputation MSE  0.46456\n",
            "-----\n",
            "Epoch 1901\n",
            "MIWAE likelihood bound  -1.41277\n",
            "Imputation MSE  0.475633\n",
            "-----\n",
            "Epoch 2001\n",
            "MIWAE likelihood bound  -1.42849\n",
            "Imputation MSE  0.443884\n",
            "-----\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZsYAmymH28c",
        "colab_type": "text"
      },
      "source": [
        "# Comparisons with other methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84kWm6OVIRq8",
        "colab_type": "text"
      },
      "source": [
        "We make use of the recent [IterativeImputer](https://scikit-learn.org/dev/auto_examples/impute/plot_iterative_imputer_variants_comparison.html) mehod implemented in scikit-learn. It allows, in particular, to use an imputation technique quite similar to the popular missForest algorithm of  [Stekhoven & BÃ¼hlmann (2011)](https://academic.oup.com/bioinformatics/article/28/1/112/219101)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQMQZzoqEXxw",
        "colab_type": "code",
        "outputId": "1d088f73-e981-4204-822d-cdeededc06ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "missforest = IterativeImputer(max_iter=20, estimator=ExtraTreesRegressor(n_estimators=100))\n",
        "iterative_ridge = IterativeImputer(max_iter=20, estimator=BayesianRidge())\n",
        "missforest.fit(xmiss)\n",
        "iterative_ridge.fit(xmiss)\n",
        "xhat_mf = missforest.transform(xmiss)\n",
        "xhat_ridge = iterative_ridge.transform(xmiss)\n",
        "mean_imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "mean_imp.fit(xmiss)\n",
        "xhat_mean = mean_imp.transform(xmiss)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/impute/_iterative.py:599: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
            "  \" reached.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/impute/_iterative.py:599: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
            "  \" reached.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PF3VgTEGEXu-",
        "colab_type": "code",
        "outputId": "2d9bbffb-8f95-4081-9b5f-6757a556851f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plt.plot(range(1,n_epochs,100),mse_train,color=\"blue\")\n",
        "plt.axhline(y=mse(xhat_mf,xfull,mask),  linestyle='-',color=\"red\")\n",
        "plt.axhline(y=mse(xhat_ridge,xfull,mask),  linestyle='-',color=\"orange\")\n",
        "plt.axhline(y=mse(xhat_mean,xfull,mask),  linestyle='-',color=\"green\")\n",
        "plt.legend([\"MIWAE\",\"missForest\",\"Iterative ridge\", \"Mean imputation\"])\n",
        "plt.title(\"Imputation MSE\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEWCAYAAACdaNcBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACoF-oZACqBi",
        "colab_type": "text"
      },
      "source": [
        "# Exercise:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMCyAOVmCr11",
        "colab_type": "text"
      },
      "source": [
        "Try to come up with a better single imputation estimate, and implement it!"
      ]
    }
  ]
}